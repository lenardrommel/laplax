{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "350dcb93",
   "metadata": {},
   "source": [
    "# FSP Laplace: Two Moons Classification\n",
    "\n",
    "This notebook demonstrates FSP (Function-Space Prior) Laplace approximation vs Standard Laplace \n",
    "for binary classification on the two moons dataset **with zero noise** to show clean decision boundaries.\n",
    "\n",
    "The notebook compares:\n",
    "- **FSP Laplace**: Function-space prior with GP kernel\n",
    "- **Standard Laplace**: Parameter-space Laplace approximation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0169ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax import nnx\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "from laplax.api import GGN\n",
    "from laplax.curv import (\n",
    "    KernelStructure,\n",
    "    create_fsp_posterior,\n",
    "    estimate_curvature,\n",
    "    set_posterior_fn,\n",
    ")\n",
    "from laplax.enums import CurvApprox, LossFn\n",
    "from laplax.util.flatten import create_pytree_flattener\n",
    "\n",
    "# jax.config.update(\"jax_enable_x64\", True)  # Disabled to avoid dtype issues\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c7cdda",
   "metadata": {},
   "source": [
    "## Model Definition (Flax NNX)\n",
    "\n",
    "We use a simple MLP with Flax NNX for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b733a92",
   "metadata": {},
   "outputs": [],
   "source": "class MLP(nnx.Module):\n    \"\"\"Simple MLP for binary classification.\"\"\"\n\n    def __init__(self, hidden_dims: list[int], *, rngs: nnx.Rngs):\n        in_dim = 2  # Two moons dataset has 2 features\n        \n        # Build layers - use attributes to avoid tuple storage issue\n        for i, hidden_dim in enumerate(hidden_dims):\n            setattr(self, f\"hidden_{i}\", nnx.Linear(in_dim, hidden_dim, rngs=rngs))\n            in_dim = hidden_dim\n        \n        # Output layer (single value for binary classification)\n        self.output_layer = nnx.Linear(in_dim, 1, rngs=rngs)\n        self.n_hidden = len(hidden_dims)\n\n    def __call__(self, x: jax.Array) -> jax.Array:\n        for i in range(self.n_hidden):\n            x = jnp.tanh(getattr(self, f\"hidden_{i}\")(x))\n        x = self.output_layer(x)\n        return x.squeeze()\n\n\ndef split_model(model: MLP):\n    \"\"\"Split Flax NNX model into function and parameters.\"\"\"\n    graphdef, params = nnx.split(model, nnx.Param)\n    def model_fn(x, params):\n        model_copy = nnx.merge(graphdef, params)\n        return model_copy(x)\n    return model_fn, params"
  },
  {
   "cell_type": "markdown",
   "id": "7194fd15",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bb2630",
   "metadata": {},
   "outputs": [],
   "source": "def binary_cross_entropy_loss(model, x_batch, y_batch):\n    \"\"\"Binary cross-entropy loss.\"\"\"\n    logits = jax.vmap(model)(x_batch)\n    return -jnp.mean(jax.nn.log_sigmoid(logits) * y_batch +\n                     jax.nn.log_sigmoid(-logits) * (1 - y_batch))\n\n\ndef train_mlp(model, x_train, y_train, num_epochs=100, learning_rate=0.1):\n    \"\"\"Train MLP with Adam optimizer.\"\"\"\n    optimizer = nnx.Optimizer(model, optax.adam(learning_rate), wrt=nnx.Param)\n\n    @nnx.jit\n    def train_step(model, optimizer, x_batch, y_batch):\n        loss, grads = nnx.value_and_grad(binary_cross_entropy_loss)(model, x_batch, y_batch)\n        optimizer.update(model, grads)  # Requires Flax >= 0.11.0\n        return loss\n\n    for epoch in range(num_epochs):\n        loss = train_step(model, optimizer, x_train, y_train)\n        if (epoch + 1) % 20 == 0:\n            print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss:.4f}\")\n\n    return model"
  },
  {
   "cell_type": "markdown",
   "id": "88222217",
   "metadata": {},
   "source": [
    "## Generate Two Moons Data (Zero Noise)\n",
    "\n",
    "We generate the two moons dataset with **noise=0.0** for clean visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7809fb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate two moons dataset with ZERO noise\n",
    "X, y = make_moons(n_samples=300, noise=0.0, random_state=42)\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.float32)\n",
    "\n",
    "X_jax = jnp.array(X)\n",
    "y_jax = jnp.array(y)\n",
    "\n",
    "print(f\"Dataset shape: X={X.shape}, y={y.shape}\")\n",
    "\n",
    "# Visualize data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"RdBu_r\", edgecolor=\"k\", s=50)\n",
    "plt.title(\"Two Moons Dataset (Zero Noise)\", fontweight=\"bold\", fontsize=14)\n",
    "plt.xlabel(\"x₁\")\n",
    "plt.ylabel(\"x₂\")\n",
    "plt.colorbar(label=\"Class\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe1b08b",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abe14d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train model\n",
    "model = MLP(hidden_dims=[32, 32], rngs=nnx.Rngs(0))\n",
    "model = train_mlp(model, X_jax, y_jax, num_epochs=100, learning_rate=0.1)\n",
    "\n",
    "# Evaluate training accuracy\n",
    "logits = jax.vmap(model)(X_jax)\n",
    "predictions = (jax.nn.sigmoid(logits) > 0.5).astype(jnp.float32)\n",
    "train_acc = jnp.mean(predictions == y_jax)\n",
    "print(f\"\\nTraining accuracy: {train_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df8b3ec",
   "metadata": {},
   "source": [
    "## Compute FSP Posterior\n",
    "\n",
    "We use an RBF kernel for the function-space prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64f3d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select context points\n",
    "n_context = 50\n",
    "np.random.seed(42)\n",
    "context_indices = np.random.choice(len(X), size=n_context, replace=False)\n",
    "x_context = X_jax[context_indices]\n",
    "print(f\"Selected {x_context.shape[0]} context points\")\n",
    "\n",
    "# RBF kernel\n",
    "def rbf_kernel(x1, x2, lengthscale=1.0, variance=1.0):\n",
    "    sq_dist = jnp.sum((x1[:, None, :] - x2[None, :, :]) ** 2, axis=-1)\n",
    "    return variance * jnp.exp(-sq_dist / (2 * lengthscale**2))\n",
    "\n",
    "def create_kernel_matrix(x_context, lengthscale=1.0, variance=1.0):\n",
    "    K = rbf_kernel(x_context, x_context, lengthscale, variance)\n",
    "    return K + 1e-6 * jnp.eye(K.shape[0])\n",
    "\n",
    "# Kernel hyperparameters\n",
    "lengthscale = 0.5\n",
    "variance = 0.01\n",
    "\n",
    "def kernel_fn(v):\n",
    "    K = create_kernel_matrix(x_context, lengthscale=lengthscale, variance=variance)\n",
    "    return K @ v\n",
    "\n",
    "prior_cov = create_kernel_matrix(x_context, lengthscale=lengthscale, variance=variance)\n",
    "prior_variance = jnp.diag(prior_cov)\n",
    "\n",
    "print(f\"Prior variance range: [{prior_variance.min():.4f}, {prior_variance.max():.4f}]\")\n",
    "\n",
    "# Split model for laplax\n",
    "model_fn, trained_params = split_model(model)\n",
    "\n",
    "# Create FSP posterior\n",
    "posterior = create_fsp_posterior(\n",
    "    model_fn=model_fn,\n",
    "    params=trained_params,\n",
    "    x_context=x_context,\n",
    "    kernel_structure=KernelStructure.NONE,\n",
    "    kernel=kernel_fn,\n",
    "    prior_variance=prior_variance,\n",
    "    n_chunks=2,\n",
    "    max_iter=50,\n",
    ")\n",
    "\n",
    "print(f\"FSP posterior rank: {posterior.rank}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b256eea",
   "metadata": {},
   "source": [
    "## Compute Standard Laplace Posterior\n",
    "\n",
    "For comparison, we also compute the standard parameter-space Laplace approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48227d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from laplax.util.flatten import create_pytree_flattener\n",
    "\n",
    "flatten_fn, unflatten_fn = create_pytree_flattener(trained_params)\n",
    "\n",
    "def model_fn_flat(input, params):\n",
    "    params_pytree = unflatten_fn(params)\n",
    "    logit = model_fn(input, params_pytree)\n",
    "    return jnp.stack([jnp.zeros_like(logit), logit], axis=-1)\n",
    "\n",
    "data = {\"input\": X_jax, \"target\": y_jax.astype(jnp.int32)}\n",
    "ggn_mv = GGN(\n",
    "    model_fn_flat,\n",
    "    flatten_fn(trained_params),\n",
    "    data,\n",
    "    loss_fn=LossFn.CROSS_ENTROPY,\n",
    "    vmap_over_data=True,\n",
    ")\n",
    "\n",
    "max_rank = 50\n",
    "curv_estimate = estimate_curvature(\n",
    "    curv_type=CurvApprox.LANCZOS,\n",
    "    mv=ggn_mv,\n",
    "    layout=flatten_fn(trained_params).shape[0],\n",
    "    rank=max_rank,\n",
    "    key=jax.random.key(42),\n",
    "    has_batch=True,\n",
    ")\n",
    "\n",
    "posterior_fn = set_posterior_fn(\n",
    "    curv_type=CurvApprox.LANCZOS,\n",
    "    curv_estimate=curv_estimate,\n",
    "    layout=flatten_fn(trained_params).shape[0],\n",
    ")\n",
    "\n",
    "standard_posterior = posterior_fn({\"prior_prec\": 1.0})\n",
    "print(f\"Standard Laplace rank: {curv_estimate.U.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b0cd9c",
   "metadata": {},
   "source": [
    "## Make Predictions with Uncertainty\n",
    "\n",
    "Sample from both posteriors to quantify uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1609a5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "grid_jax = jnp.array(grid_points)\n",
    "\n",
    "# Mean predictions\n",
    "mean_logits = jax.vmap(model)(grid_jax)\n",
    "mean_probs = jax.nn.sigmoid(mean_logits)\n",
    "\n",
    "print(\"Sampling from FSP posterior...\")\n",
    "key = jax.random.PRNGKey(42)\n",
    "n_samples = 20\n",
    "fsp_samples = []\n",
    "\n",
    "for i in range(n_samples):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    z = jax.random.normal(subkey, (posterior.rank,))\n",
    "    delta_params = posterior.scale_mv(posterior.state)(z)\n",
    "    sample_params = jax.tree.map(lambda p, dp: p + dp, trained_params, delta_params)\n",
    "    sample_logits = jax.vmap(lambda x: model_fn(x, sample_params))(grid_jax)\n",
    "    fsp_samples.append(jax.nn.sigmoid(sample_logits))\n",
    "\n",
    "fsp_samples = jnp.stack(fsp_samples)\n",
    "fsp_std = jnp.std(fsp_samples, axis=0)\n",
    "\n",
    "print(\"Sampling from standard Laplace posterior...\")\n",
    "key = jax.random.PRNGKey(42)\n",
    "standard_samples = []\n",
    "param_size = flatten_fn(trained_params).shape[0]\n",
    "\n",
    "for i in range(n_samples):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    z = jax.random.normal(subkey, (param_size,))\n",
    "    delta_params_flat = standard_posterior.scale_mv(standard_posterior.state)(z)\n",
    "    delta_params = unflatten_fn(delta_params_flat)\n",
    "    sample_params = jax.tree.map(lambda p, dp: p + dp, trained_params, delta_params)\n",
    "    sample_logits = jax.vmap(lambda x: model_fn(x, sample_params))(grid_jax)\n",
    "    standard_samples.append(jax.nn.sigmoid(sample_logits))\n",
    "\n",
    "standard_samples = jnp.stack(standard_samples)\n",
    "standard_std = jnp.std(standard_samples, axis=0)\n",
    "\n",
    "print(f\"FSP uncertainty - Std range: [{fsp_std.min():.3f}, {fsp_std.max():.3f}]\")\n",
    "print(f\"Standard uncertainty - Std range: [{standard_std.min():.3f}, {standard_std.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2336bc",
   "metadata": {},
   "source": [
    "## Visualization: FSP vs Standard Laplace\n",
    "\n",
    "Compare the decision boundaries and uncertainty estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a104e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Row 1: FSP Laplace\n",
    "ax = axes[0, 0]\n",
    "contour = ax.contourf(xx, yy, mean_probs.reshape(xx.shape), levels=20, cmap=\"RdBu_r\", alpha=0.8)\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"RdBu_r\", edgecolor=\"k\", s=30, alpha=0.6)\n",
    "ax.scatter(x_context[:, 0], x_context[:, 1], c=\"green\", marker=\"x\", s=100,\n",
    "           label=\"Context points\", linewidths=2)\n",
    "ax.set_title(\"Mean Predictions (Both Methods)\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_xlabel(\"x₁\")\n",
    "ax.set_ylabel(\"x₂\")\n",
    "ax.legend()\n",
    "plt.colorbar(contour, ax=ax)\n",
    "\n",
    "ax = axes[0, 1]\n",
    "contour = ax.contourf(xx, yy, fsp_std.reshape(xx.shape), levels=20, cmap=\"viridis\", alpha=0.8)\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"RdBu_r\", edgecolor=\"k\", s=30, alpha=0.6)\n",
    "ax.set_title(f\"FSP Laplace Uncertainty (rank={posterior.rank})\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_xlabel(\"x₁\")\n",
    "ax.set_ylabel(\"x₂\")\n",
    "plt.colorbar(contour, ax=ax, label=\"Std\")\n",
    "\n",
    "ax = axes[0, 2]\n",
    "fsp_confidence = 1 - 2 * fsp_std\n",
    "contour = ax.contourf(xx, yy, fsp_confidence.reshape(xx.shape), levels=20, cmap=\"plasma\", alpha=0.8)\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"RdBu_r\", edgecolor=\"k\", s=30, alpha=0.6)\n",
    "ax.set_title(\"FSP Laplace Confidence\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_xlabel(\"x₁\")\n",
    "ax.set_ylabel(\"x₂\")\n",
    "plt.colorbar(contour, ax=ax, label=\"Confidence\")\n",
    "\n",
    "# Row 2: Standard Laplace\n",
    "ax = axes[1, 0]\n",
    "contour = ax.contourf(xx, yy, mean_probs.reshape(xx.shape), levels=20, cmap=\"RdBu_r\", alpha=0.8)\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"RdBu_r\", edgecolor=\"k\", s=30, alpha=0.6)\n",
    "ax.set_title(\"Mean Predictions (Both Methods)\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_xlabel(\"x₁\")\n",
    "ax.set_ylabel(\"x₂\")\n",
    "plt.colorbar(contour, ax=ax)\n",
    "\n",
    "ax = axes[1, 1]\n",
    "contour = ax.contourf(xx, yy, standard_std.reshape(xx.shape), levels=20, cmap=\"viridis\", alpha=0.8)\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"RdBu_r\", edgecolor=\"k\", s=30, alpha=0.6)\n",
    "ax.set_title(f\"Standard Laplace Uncertainty (rank={curv_estimate.U.shape[1]})\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_xlabel(\"x₁\")\n",
    "ax.set_ylabel(\"x₂\")\n",
    "plt.colorbar(contour, ax=ax, label=\"Std\")\n",
    "\n",
    "ax = axes[1, 2]\n",
    "standard_confidence = 1 - 2 * standard_std\n",
    "contour = ax.contourf(xx, yy, standard_confidence.reshape(xx.shape), levels=20, cmap=\"plasma\", alpha=0.8)\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=\"RdBu_r\", edgecolor=\"k\", s=30, alpha=0.6)\n",
    "ax.set_title(\"Standard Laplace Confidence\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_xlabel(\"x₁\")\n",
    "ax.set_ylabel(\"x₂\")\n",
    "plt.colorbar(contour, ax=ax, label=\"Confidence\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb6fa5c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Both FSP and Standard Laplace produce clean S-shaped decision boundaries on the zero-noise two moons dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b04495",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"Comparison Summary:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"FSP Laplace:\")\n",
    "print(f\"  - Rank: {posterior.rank}\")\n",
    "print(f\"  - Mean uncertainty: {fsp_std.mean():.4f}\")\n",
    "print(f\"  - Max uncertainty: {fsp_std.max():.4f}\")\n",
    "print(f\"\\nStandard Laplace:\")\n",
    "print(f\"  - Rank: {curv_estimate.U.shape[1]}\")\n",
    "print(f\"  - Mean uncertainty: {standard_std.mean():.4f}\")\n",
    "print(f\"  - Max uncertainty: {standard_std.max():.4f}\")\n",
    "print(f\"\\nUncertainty difference (FSP - Standard):\")\n",
    "print(f\"  - Mean: {(fsp_std - standard_std).mean():.4f}\")\n",
    "print(f\"  - Max absolute: {jnp.abs(fsp_std - standard_std).max():.4f}\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}