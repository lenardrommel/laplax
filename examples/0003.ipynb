{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9f319f7",
   "metadata": {},
   "source": [
    "# Introduction to `laplax`s FSP-Laplace regression tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2c77f2",
   "metadata": {},
   "source": [
    "This tutorial follows one of the toy data experiments of [FSP-Laplace: Function-Space Priors for the Laplace Approximation in Bayesian Deep Learning](https://arxiv.org/abs/2407.13711) for regression and provides a quick overview of the FSP Laplace approximation in jax.\n",
    "We regress on data that is modelled by:\n",
    "$y = \\sin(2\\pi x) + \\mathcal N(0, \\sigma_n^2=0.1),$ on the intervals $[-1, -0.5] \\cup [0.5, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602aa8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax import nnx\n",
    "from helper import DataLoader, get_sinusoid_example, to_float64\n",
    "from plotting import plot_regression_with_uncertainty, plot_sinusoid_task\n",
    "from prior import *\n",
    "\n",
    "import laplax\n",
    "from laplax import util\n",
    "from laplax.extra.fsp import *\n",
    "from laplax.extra.fsp import lanczos_isqrt\n",
    "from laplax.extra.fsp.fsp import compute_matrix_jacobian_product\n",
    "from laplax.util.tree import to_dtype\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e635cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "key = jax.random.key(0)\n",
    "X_train1 = jnp.linspace(-1, -0.5, 75).reshape(-1, 1)\n",
    "X_train2 = jnp.linspace(0.5, 1, 75).reshape(-1, 1)\n",
    "X_train = jnp.concatenate([X_train1, X_train2], axis=0)\n",
    "y_train = jnp.reshape(jnp.sin(X_train * 2 * jnp.pi) + jax.random.normal(key, (150, 1)) * 0.1, (-1, 1))\n",
    "X_test = X_train\n",
    "y_test = jnp.reshape(jnp.sin(X_train * 2 * jnp.pi) + jax.random.normal(key, (150, 1)) * 0.2, (-1, 1))\n",
    "train_loader = DataLoader(X_train, y_train, batch_size)\n",
    "data = {\"input\": X_train, \"target\": y_train}\n",
    "\n",
    "fig = plot_sinusoid_task(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481c77e6",
   "metadata": {},
   "source": [
    "## Training for the MAP and defining the GP prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8814c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nnx.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, rngs):\n",
    "        self.linear1 = nnx.Linear(in_channels, hidden_channels, rngs=rngs)\n",
    "        self.linear2 = nnx.Linear(hidden_channels, hidden_channels, rngs=rngs)\n",
    "        self.linear3 = nnx.Linear(hidden_channels, out_channels, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.linear3(\n",
    "            nnx.tanh(self.linear2(\n",
    "                nnx.tanh(self.linear1(x)))\n",
    "                )\n",
    "            )\n",
    "        return x\n",
    "    \n",
    "\n",
    "class MLP(nnx.Module):\n",
    "    def __init__(self, model, param=None):\n",
    "        self.model = model\n",
    "        if param is not None:\n",
    "            self.scale = nnx.Param(jnp.asarray(param))\n",
    "        else:\n",
    "            self.scale = nnx.Param(jnp.array(jnp.log(1 - jnp.exp(-0.1))))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    \n",
    "model = Model(in_channels=1, hidden_channels=50, out_channels=1, rngs=nnx.Rngs(2))\n",
    "model = to_float64(model)\n",
    "model = MLP(model)\n",
    "\n",
    "graph_def, params = nnx.split(model)\n",
    "\n",
    "def model_fn(input, params):\n",
    "    return nnx.call((graph_def, params))(input)[0]\n",
    "\n",
    "prior_params = {\n",
    "    # \"per_ls\": 2.947,\n",
    "    # \"per_p\": 1.0,\n",
    "    # \"per_var\": 6.608,\n",
    "    # \"matern52_ls\": 143.478,\n",
    "    \"per_ls\": 1.0,\n",
    "    \"per_p\": 1.0,\n",
    "    \"per_var\": 0.5,\n",
    "    \"matern52_ls\": 4.0,\n",
    "    \"matern12_ls\": 0.1,\n",
    "    \"matern12_var\": 0.0,  # 0.25,\n",
    "} \n",
    "\n",
    "\n",
    "def kernel_fn(xc):\n",
    "    return gram(xc, prior_params, composite_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ce01b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca424766",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_plot = jnp.linspace(-3, 3, 200)[:, None]\n",
    "\n",
    "K = kernel_fn(X_plot)\n",
    "\n",
    "K_sqrt = jax.scipy.linalg.sqrtm((K + K.T) / 2)\n",
    "\n",
    "sample = K_sqrt @ jax.random.normal(jax.random.key(10), (K.shape[0], 5))\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(X_plot, sample, color=\"C0\", alpha=0.1)\n",
    "plt.plot(X_plot, jnp.sin(X_plot * 2 * jnp.pi), color=\"red\", linestyle=\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa49663",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.jit(static_argnames=['loss_fn'])\n",
    "def train_step(model, data, x_context, loss_fn):\n",
    "    def loss_function(model):\n",
    "        graph_def, current_params = nnx.split(model)\n",
    "        \n",
    "        def wrapped_loss_fn(data, x_context, params):\n",
    "            \n",
    "            temp_model = nnx.merge(graph_def, params)\n",
    "            return loss_fn(data, x_context, params, jax.nn.softplus(temp_model.scale.value))\n",
    "        \n",
    "        return wrapped_loss_fn(data, x_context, current_params)\n",
    "    \n",
    "    loss, grads = nnx.value_and_grad(loss_function)(model)\n",
    "    return loss, grads\n",
    "\n",
    "def train_model(model, n_epochs, lr=1e-2):\n",
    "    optimizer = nnx.Optimizer(model, optax.adam(lr))\n",
    "    graph_def, _ = nnx.split(model)\n",
    "    \n",
    "    def model_fn(input, params):\n",
    "        return nnx.call((graph_def, params))(input)[0]\n",
    "    \n",
    "    loss_fn = create_fsp_objective(model_fn, X_train.shape[0], jnp.zeros((200, 1)), kernel_fn)  # noqa: F405\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for x_tr, y_tr in train_loader:\n",
    "            data = {\"input\": x_tr, \"target\": y_tr}\n",
    "            x_context = jnp.linspace(-2, 2, 200).reshape(-1, 1)\n",
    "            \n",
    "            loss, grads = train_step(model, data, x_context, loss_fn)\n",
    "            optimizer.update(grads)\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"[epoch {epoch}]: loss: {loss:.4f} Scale: {jax.nn.softplus(model.scale.value):.4f}\")\n",
    "            \n",
    "    print(f\"Final loss: {loss:.4f}\")\n",
    "    return model\n",
    "\n",
    "model = train_model(model, n_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e80a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = jnp.linspace(-2., 2., 200).reshape(200, 1)\n",
    "y_pred = jax.vmap(model)(X_pred)\n",
    "\n",
    "_ = plot_sinusoid_task(X_train, y_train, X_test, y_test, X_pred, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062d8e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph_def, params = nnx.split(model.model)\n",
    "\n",
    "def model_fn(input, params):\n",
    "    return nnx.call((graph_def, params))(input)[0]\n",
    "\n",
    "context_points = select_context_points(1000, \"grid\", [3.0], [-3.0], X_train.shape, key=jax.random.key(0))\n",
    "prob_predictive = fsp_laplace(model_fn, params, data, kernel_fn, context_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d379f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = jnp.linspace(-2, 2, 200, dtype=jnp.float64).reshape(-1, 1)\n",
    "\n",
    "pred = jax.vmap(prob_predictive)(X_pred)\n",
    "plot_regression_with_uncertainty(\n",
    "        X_train=data[\"input\"],\n",
    "        y_train=data[\"target\"],\n",
    "        X_pred=X_pred,\n",
    "        y_pred=pred[\"pred_mean\"][:, 0],\n",
    "        y_std=jnp.sqrt(pred[\"pred_var\"][:, 0]),\n",
    "        y_samples=pred[\"samples\"],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6430338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccc7b15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
