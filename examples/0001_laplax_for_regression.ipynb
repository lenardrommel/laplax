{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "# Introduction to `laplax` for regression tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial follows the `laplace-torch` regression tutorial and provides a quick overview of the different functionalities which are currently supported by `laplax`.\n",
    "\n",
    "For the dataset we consider sinus as our target with additional observation noise $\\sigma^2 = 0.3$. To make the task harder, we only consider training and validation data on a few subintervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax import nnx\n",
    "from helper import DataLoader, get_sinusoid_example\n",
    "from plotting import plot_sinusoid_task\n",
    "\n",
    "n_epochs = 1000\n",
    "key = jax.random.key(0)\n",
    "\n",
    "# Sample toy data example\n",
    "num_training_samples = 150\n",
    "num_calibration_samples = 50\n",
    "num_test_samples = 150\n",
    "\n",
    "batch_size = 20\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test = get_sinusoid_example(\n",
    "    num_train_data=num_training_samples,\n",
    "    num_valid_data=num_calibration_samples,\n",
    "    num_test_data=num_test_samples,\n",
    "    sigma_noise=0.3,\n",
    "    intervals=[(0, 2), (4, 5), (6, 8)],\n",
    "    rng_key=jax.random.key(0),\n",
    ")\n",
    "train_loader = DataLoader(X_train, y_train, batch_size)\n",
    "\n",
    "fig = plot_sinusoid_task(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    }
   },
   "source": [
    "## Training for the MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we use `flax.nnx` for setting up neural networks in `jax`, but other libraries (e.g., `equinox` or `flax.linen`) should also work out of the box since we will only require a split into `model_fn` and `params` for `laplax`, which all of them provide.\n",
    "\n",
    "From a Bayesian perspective supervised learning can be seen as finding the maximum-a-posteriori estimate of the joint log likelihood:\n",
    "\n",
    "$$ \\text{arg}\\max_{\\theta_\\in\\mathbb{R}^{P}} = \\sum_{n=1}^{N} \\log p(y_n \\vert f(x_n, \\theta) )+ \\log p(\\theta) $$\n",
    "\n",
    "where:\n",
    "- $f$ is the neural network,\n",
    "- $\\theta \\in \\mathbb{R}^{P}$ its parameters, and\n",
    "- $\\mathcal{D} := \\{(x_n, y_n)\\}_{n=1}^{N}$ the labelled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train MAP model\n",
    "class Model(nnx.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, rngs):\n",
    "        self.linear1 = nnx.Linear(in_channels, hidden_channels, rngs=rngs)\n",
    "        self.linear2 = nnx.Linear(hidden_channels, out_channels, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.linear2(nnx.tanh(self.linear1(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(model, optimizer, x, y):\n",
    "    def loss_fn(model):\n",
    "        y_pred = model(x)  # Call methods directly\n",
    "        return jnp.sum((y_pred - y) ** 2)\n",
    "\n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(model)\n",
    "    optimizer.update(grads)  # Inplace updates\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_model(model, n_epochs, lr=1e-3):\n",
    "    # Create optimizer\n",
    "    optimizer = nnx.Optimizer(model, optax.adamw(lr))  # Reference sharing\n",
    "\n",
    "    # Train epoch\n",
    "    for epoch in range(n_epochs):\n",
    "        for x_tr, y_tr in train_loader:\n",
    "            loss = train_step(model, optimizer, x_tr, y_tr)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"[epoch {epoch}]: loss: {loss:.4f}\")\n",
    "\n",
    "    print(f\"Final loss: {loss:.4f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = Model(in_channels=1, hidden_channels=64, out_channels=1, rngs=nnx.Rngs(0))\n",
    "\n",
    "# Train model\n",
    "model = train_model(model, n_epochs=1000)\n",
    "\n",
    "X_pred = jnp.linspace(0.0, 8.0, 200).reshape(200, 1)\n",
    "y_pred = jax.vmap(model)(X_pred)\n",
    "\n",
    "_ = plot_sinusoid_task(X_train, y_train, X_test, y_test, X_pred, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the curvature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now interested in finding a normal distribution that describes the uncertainty in the weight space with respect to the loss and the data:\n",
    "$$ p(\\theta \\vert \\mathcal{D}) = \\frac{p(\\mathcal{D}, \\theta)}{p(\\mathcal{D})} = \\frac{p(\\mathcal{D} \\vert \\theta) p(\\theta)}{\\int p(\\mathcal{D} \\vert \\theta) p(\\theta) d\\theta}. $$\n",
    "\n",
    "Our tool of choice is the Laplace approximation --- motivated via a second-order Taylor expansion, where the first-order term disappears due to the assumption of having reached a local minimum of the loss. Following these steps we get the following normal distribution approximating the true posterior:\n",
    "$$ \\theta \\sim \\mathcal{N}(\\theta_{\\text{MAP}}, [\\nabla^2_{\\theta\\theta} \\log p(\\theta \\vert \\mathcal{D}) \\vert_{\\theta = \\theta_{\\text{MAP}}} ]^{-1}). $$\n",
    "\n",
    "We usually assume the prior to be an isotropic Gaussian distribution, hence the expensive part remains mainly the loss hessian. Due to various reasons (positive definiteness or/and a linearized perspective of the neural network) we usually consider instead of the true Hessian the so-called Generalized-Gauss Newton matrix:\n",
    "\n",
    "$$ \\text{GGN}(f, \\theta, \\mathcal{D}) = \\sum_{n=1}^{N} \\mathcal{J}_\\theta f(x_n)^\\top \\nabla^2_{\\theta\\theta}\\ell(f_\\theta(x_n), y_n) \\mathcal{J}_\\theta f(x_n).$$ \n",
    "\n",
    "We start by splitting the `flax.nnx` model into `model_fn` and `params`. **Important** The signature of the `model_fn` needs to be `input` and `params`, since we strongly depend on the key word arguments in `laplax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from laplax.curv import create_ggn_mv\n",
    "from laplax.util.loader import input_target_split\n",
    "\n",
    "# Create GGN\n",
    "graph_def, params = nnx.split(model)\n",
    "\n",
    "\n",
    "def model_fn(input, params):\n",
    "    return nnx.call((graph_def, params))(input)[0]\n",
    "\n",
    "\n",
    "train_batch = {\"input\": X_train, \"target\": y_train}\n",
    "\n",
    "ggn_mv = create_ggn_mv(\n",
    "    model_fn,\n",
    "    params,\n",
    "    train_batch,\n",
    "    loss_fn=\"mse\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this small toy example, we can dense the curvature matrix-vector product. We start by wrapping the matrix-vector product to accept normal 1D vectors of size $P$. This will help us visualize the GGN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import SymLogNorm\n",
    "\n",
    "from laplax.util.flatten import flatten_function\n",
    "from laplax.util.mv import to_dense\n",
    "from laplax.util.tree import get_size\n",
    "\n",
    "ggn_mv_wrapped = flatten_function(ggn_mv, layout=params)\n",
    "arr = to_dense(ggn_mv_wrapped, layout=get_size(params))\n",
    "\n",
    "\n",
    "plt.imshow(arr, norm=SymLogNorm(linthresh=1e-2, linscale=1))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curvature estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we can not afford to dense and continue computations with the GGN. Therefore, various strategies for estimating the curvature exist. Within this package we have: `full` (obvious), `diagonal` and low_rank. For the latter, we support finding the low rank representation using `lanczos` or `lobpcg`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Create dropdown for library selection.\n",
    "lib_dropdown = widgets.Dropdown(\n",
    "    options=[\"full\", \"diagonal\", \"lanczos\", \"lobpcg\"],\n",
    "    value=\"full\",\n",
    "    description=\"Curv. est.:\",\n",
    ")\n",
    "display(lib_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from laplax.curv import estimate_curvature\n",
    "\n",
    "print(f\"Curvature will be estimated using a {lib_dropdown.value} approximation.\")\n",
    "curv_type = lib_dropdown.value\n",
    "low_rank_args = {\n",
    "    \"key\": jax.random.key(20),\n",
    "    \"rank\": 50,\n",
    "    \"mv_jit\": True,\n",
    "}\n",
    "curv_args = {} if curv_type in {\"full\", \"diagonal\"} else low_rank_args\n",
    "\n",
    "curv_estimate = estimate_curvature(\n",
    "    curv_type=curv_type,\n",
    "    mv=ggn_mv,\n",
    "    layout=params,\n",
    "    **curv_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a posterior_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a `posterior_fn` that takes `prior_arguments` and returns a posterior distribution over the weights. This includes adding the prior precision $\\tau$ and inverting the combined expression in a memory-efficient way:\n",
    "$$ \\text{posterior\\_fn}(\\tau) = \\big( GGN + \\tau I_{P\\times P} \\big)^{-1} $$\n",
    "If we have already an estimation of the curvature, then we can directly set the posterior function using the estimate. Otherwise both functions can also be executed at once using the `laplax.curv.create_posterior_fn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from laplax.curv.cov import set_posterior_fn\n",
    "\n",
    "posterior_fn = set_posterior_fn(curv_type, curv_estimate, layout=params)\n",
    "\n",
    "# # Alternatively, we can create the posterior function from scratch, if no curvature\n",
    "# # estimation is available.\n",
    "# # Create Posterior\n",
    "# posterior_fn = create_posterior_fn(\n",
    "#     curv_type=curv_type,\n",
    "#     mv=ggn_mv,\n",
    "#     layout=params,\n",
    "#     **curv_args,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to pushforward the weight space uncertainty?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ideas for pushing forward weight space uncertainty.\n",
    "\n",
    "1. Sample-based pushforward via the neural network\n",
    "$$ f(x_n, \\theta_s), \\quad \\theta_s \\sim \\mathcal{N}\\bigg(\\theta_{MAP}, \\Sigma\\bigg)$$\n",
    "\n",
    "2. Linearized pushforward\n",
    "$$ f(x_n, \\theta) \\sim \\mathcal{N}\\bigg(f(x_n, \\theta_{MAP}), \\mathcal{J}_{\\theta}(f(x_n, \\theta_{\\text{MAP}}))\\Sigma \\mathcal{J}_{\\theta}(f(x_n, \\theta_{\\text{MAP}}))^\\top\\bigg)$$\n",
    "\n",
    "**Recommendation:** Play around with the prior precision to see its strong modeling impact. Also check out larger intervals to see the uncertainty structure outside of the training domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample-based pushforward via the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from plotting import plot_regression_with_uncertainty\n",
    "\n",
    "from laplax.eval.pushforward import (\n",
    "    nonlin_pred_mean,\n",
    "    nonlin_pred_std,\n",
    "    nonlin_pred_var,\n",
    "    nonlin_setup,\n",
    "    set_nonlin_pushforward,\n",
    ")\n",
    "\n",
    "# Setup linearized pushforward\n",
    "set_nonlin_prob_predictive = partial(\n",
    "    set_nonlin_pushforward,\n",
    "    model_fn=model_fn,\n",
    "    mean_params=params,\n",
    "    posterior_fn=posterior_fn,\n",
    "    pushforward_fns=[nonlin_setup, nonlin_pred_mean, nonlin_pred_var, nonlin_pred_std],\n",
    "    key=jax.random.key(42),\n",
    "    num_samples=10000,\n",
    ")\n",
    "prior_arguments = {\"prior_prec\": 40.0}  # Choose any prior precision.\n",
    "prob_predictive = set_nonlin_prob_predictive(\n",
    "    prior_arguments=prior_arguments,\n",
    ")\n",
    "\n",
    "X_pred = jnp.linspace(0, 8, 200).reshape(200, 1)\n",
    "pred = jax.vmap(prob_predictive)(X_pred)\n",
    "\n",
    "_ = plot_regression_with_uncertainty(\n",
    "    X_train=train_batch[\"input\"],\n",
    "    y_train=train_batch[\"target\"],\n",
    "    X_pred=X_pred,\n",
    "    y_pred=pred[\"pred_mean\"][:, 0],\n",
    "    y_std=jnp.sqrt(pred[\"pred_var\"][:, 0]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearized pushforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from plotting import plot_regression_with_uncertainty\n",
    "\n",
    "from laplax.eval.pushforward import (\n",
    "    lin_pred_mean,\n",
    "    lin_pred_std,\n",
    "    lin_pred_var,\n",
    "    lin_setup,\n",
    "    set_lin_pushforward,\n",
    ")\n",
    "\n",
    "# Setup linearized pushforward\n",
    "set_prob_predictive = partial(\n",
    "    set_lin_pushforward,\n",
    "    model_fn=model_fn,\n",
    "    mean_params=params,\n",
    "    posterior_fn=posterior_fn,\n",
    "    pushforward_fns=[\n",
    "        lin_setup,\n",
    "        lin_pred_mean,\n",
    "        lin_pred_var,\n",
    "        lin_pred_std,\n",
    "    ],\n",
    ")\n",
    "prior_arguments = {\"prior_prec\": 1.0}  # Choose any prior precision.\n",
    "prob_predictive = set_prob_predictive(\n",
    "    prior_arguments=prior_arguments,\n",
    ")\n",
    "\n",
    "X_pred = jnp.linspace(0, 8, 200).reshape(200, 1)\n",
    "pred = jax.vmap(prob_predictive)(X_pred)\n",
    "\n",
    "_ = plot_regression_with_uncertainty(\n",
    "    X_train=train_batch[\"input\"],\n",
    "    y_train=train_batch[\"target\"],\n",
    "    X_pred=X_pred,\n",
    "    y_pred=pred[\"pred_mean\"][:, 0],\n",
    "    y_std=jnp.sqrt(pred[\"pred_var\"][:, 0]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When playing around we see that it is non-trivial of how to choose the prior precision. To do so with an heuristic we need to optimize some objective. There are two common strategies: either optimize for a downstream metric (e.g. Negative-Log Likelihood or average calibration ($\\chi^2$)) or target the marginal log-likelihood. Later is a common objective for even more general model selection (see below) and is given by:\n",
    "\n",
    "$$ \\log p(\\mathcal{D}\\vert\\mathcal{M}) = \\log p(\\mathcal{D} \\vert \\theta_{*}, \\mathcal{M}) + \\log p(\\mathcal{\\theta_*} \\vert \\mathcal{M}) - \\frac{1}{2} \\log \\vert \\frac{1}{2\\pi} \\mathrm{H}_{\\theta_*}\\vert $$\n",
    "\n",
    "where $\\mathrm{H}_{\\theta_*}$ is the posterior precision and $\\mathcal{M}$ other model parameters, such as the network architecture. We note that no inversion is needed compute the marginal log likelihood when updating the prior arguments. However, in practice optimizing for downstream metrics will also lead to better downstream metrics.\n",
    "\n",
    "For optimization we can choose either grid search or gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partially initializing `set_prob_predictive`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We always start by partially initializing the `set_prob_predictive`, such that it only misses the `prior_arguments`, which we will use for optimizing a chosen objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from laplax.eval.pushforward import (\n",
    "    lin_pred_mean,\n",
    "    lin_pred_std,\n",
    "    lin_setup,\n",
    "    set_lin_pushforward,\n",
    ")\n",
    "\n",
    "set_prob_predictive = partial(\n",
    "    set_lin_pushforward,\n",
    "    model_fn=model_fn,\n",
    "    mean_params=params,\n",
    "    posterior_fn=posterior_fn,\n",
    "    pushforward_fns=[\n",
    "        lin_setup,\n",
    "        lin_pred_mean,\n",
    "        lin_pred_std,\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Set a batch of calibration data\n",
    "clbr_batch = {\"input\": X_valid, \"target\": y_valid}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select calibration objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Create dropdown for library selection.\n",
    "clbr_obj_dropdown = widgets.Dropdown(\n",
    "    options=[\"nll\", \"chi_squared\", \"marginal log-likelihood\"],\n",
    "    value=\"nll\",\n",
    "    description=\"Objective:\",\n",
    ")\n",
    "display(clbr_obj_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from laplax.eval import evaluate_for_given_prior_arguments, marginal_log_likelihood\n",
    "from laplax.eval.metrics import chi_squared_zero, nll_gaussian\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def nll_objective(prior_arguments, batch):\n",
    "    return evaluate_for_given_prior_arguments(\n",
    "        prior_arguments=prior_arguments,\n",
    "        data=batch,\n",
    "        set_prob_predictive=set_prob_predictive,\n",
    "        metric=nll_gaussian,\n",
    "    )\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def chi_squared_objective(prior_arguments, batch):\n",
    "    return evaluate_for_given_prior_arguments(\n",
    "        prior_arguments=prior_arguments,\n",
    "        data=batch,\n",
    "        set_prob_predictive=set_prob_predictive,\n",
    "        metric=chi_squared_zero,\n",
    "        # This is chi_squared tansformed to have its optimal value at zero.\n",
    "    )\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def marginal_log_likelihood_objective(prior_arguments, batch):\n",
    "    return -marginal_log_likelihood(\n",
    "        curv_estimate,\n",
    "        prior_arguments=prior_arguments,\n",
    "        data=batch,\n",
    "        model_fn=model_fn,\n",
    "        params=params,\n",
    "        loss_fn=\"mse\",\n",
    "        curv_type=curv_type,\n",
    "    )\n",
    "\n",
    "\n",
    "# Select objective based on dropdown menu\n",
    "objective = {\n",
    "    \"nll\": nll_objective,\n",
    "    \"chi_squared\": chi_squared_objective,\n",
    "    \"marginal log-likelihood\": marginal_log_likelihood_objective,\n",
    "}[clbr_obj_dropdown.value]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from laplax.eval.calibrate import optimize_prior_prec\n",
    "\n",
    "prior_prec = optimize_prior_prec(\n",
    "    objective=partial(objective, batch=clbr_batch),\n",
    "    log_prior_prec_min=-3.0,\n",
    "    log_prior_prec_max=3.0,\n",
    "    grid_size=50,\n",
    ")\n",
    "\n",
    "print(\"Calibrated prior precision: \", prior_prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a similar pipeline to evaluate an arbitrary set of metrics. A few common regression metrics are natively supported in `laplax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting import print_results\n",
    "\n",
    "from laplax.eval.metrics import DEFAULT_REGRESSION_METRICS\n",
    "from laplax.eval.utils import evaluate_metrics_on_dataset\n",
    "\n",
    "# Set test batch\n",
    "test_batch = {\"input\": X_test, \"target\": y_test}\n",
    "\n",
    "prob_predictive = set_prob_predictive(prior_arguments={\"prior_prec\": prior_prec})\n",
    "results = evaluate_metrics_on_dataset(\n",
    "    pred_fn=prob_predictive,\n",
    "    data=test_batch,\n",
    "    metrics=DEFAULT_REGRESSION_METRICS,\n",
    "    reduce=jnp.mean,  # How to aggregate metrics over batch.\n",
    ")\n",
    "\n",
    "\n",
    "# Print metrics\n",
    "print_results(results, \"Model Evaluation Metrics\")\n",
    "\n",
    "# Predict for plot\n",
    "X_pred = jnp.linspace(0, 8, 200).reshape(200, 1)\n",
    "pred = jax.vmap(prob_predictive)(X_pred)\n",
    "\n",
    "_ = plot_regression_with_uncertainty(\n",
    "    X_train=train_batch[\"input\"],\n",
    "    y_train=train_batch[\"target\"],\n",
    "    X_pred=X_pred,\n",
    "    y_pred=pred[\"pred_mean\"][:, 0],\n",
    "    y_std=jnp.sqrt(pred[\"pred_var\"][:, 0]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A major benefit from the gradient descent objective is that we can straightforwardly extend the calibration to other hyperparameters. So far, we were just able to calibrate the prior precision, which will not account properly for the additional observation noise in our regression task. To change this, we will introduce the so-called `sigma_squared` term in our objective, which will support us in modeling the model uncertainty as well. The marginal log-likelihood for the mean squared error loss is then given by:\n",
    "\n",
    "$$ \\log p(\\mathcal{D}\\vert\\mathcal{M}) = \\frac{1}{2\\sigma}\\sum_{n=1}^N (y_n - f(x_n, \\theta_*))^2 + \\tau \\|\\theta_*\\|^2 - \\frac{1}{2} \\log \\vert \\frac{1}{2\\pi} \\mathrm{H}_{\\theta_*}\\vert $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize prior arguments\n",
    "prior_arguments = {\"prior_prec\": jnp.array(1.0), \"sigma_squared\": jnp.array(0.1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set parameters\n",
    "num_clbr_epochs = 100\n",
    "lr = 1e-3\n",
    "\n",
    "# Set optimizer\n",
    "optimizer = optax.adam(lr)\n",
    "opt_state = optimizer.init(prior_arguments)\n",
    "valid_loader = DataLoader(X_valid, y_valid, batch_size=16)\n",
    "\n",
    "# Transform prior arguments, so we can optimize over all reals\n",
    "prior_arguments = jax.tree.map(jnp.log, prior_arguments)\n",
    "\n",
    "# Optimize prior arguments\n",
    "with tqdm(total=num_clbr_epochs, desc=\"Training\") as pbar:\n",
    "    for _ in range(num_clbr_epochs):\n",
    "        epoch_vals = []\n",
    "        for batch in valid_loader:\n",
    "            val, grads = jax.value_and_grad(\n",
    "                lambda p: objective(\n",
    "                    jax.tree.map(jnp.exp, p),\n",
    "                    input_target_split(batch),  # noqa: B023\n",
    "                )\n",
    "            )(prior_arguments)\n",
    "\n",
    "            # Update the parameters using the optimizer\n",
    "            updates, opt_state = optimizer.update(grads, opt_state)\n",
    "            prior_arguments = optax.apply_updates(prior_arguments, updates)\n",
    "            epoch_vals.append(val)\n",
    "\n",
    "        avg_val = sum(epoch_vals) / len(epoch_vals)\n",
    "        pbar.set_postfix({\"objective\": f\"{avg_val:.4f}\"})\n",
    "        pbar.update(1)\n",
    "\n",
    "# Transform prior arguments back\n",
    "prior_arguments = jax.tree.map(jnp.exp, prior_arguments)\n",
    "\n",
    "print(\"Final values:\", dict(prior_arguments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from laplax.eval.metrics import DEFAULT_REGRESSION_METRICS\n",
    "from laplax.eval.utils import evaluate_metrics_on_dataset\n",
    "\n",
    "prob_predictive = set_prob_predictive(prior_arguments=prior_arguments)\n",
    "results = evaluate_metrics_on_dataset(\n",
    "    pred_fn=prob_predictive,\n",
    "    data=test_batch,\n",
    "    metrics=DEFAULT_REGRESSION_METRICS,\n",
    "    reduce=jnp.mean,  # How to aggregate metrics over batch.\n",
    ")\n",
    "\n",
    "# Print metrics\n",
    "print_results(results, \"Model Evaluation Metrics\")\n",
    "\n",
    "# Predict for plot\n",
    "X_pred = jnp.linspace(0, 8, 200).reshape(200, 1)\n",
    "pred = jax.vmap(prob_predictive)(X_pred)\n",
    "\n",
    "_ = plot_regression_with_uncertainty(\n",
    "    X_train=train_batch[\"input\"],\n",
    "    y_train=train_batch[\"target\"],\n",
    "    X_pred=X_pred,\n",
    "    y_pred=pred[\"pred_mean\"][:, 0],\n",
    "    y_std=jnp.sqrt(pred[\"pred_var\"][:, 0]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Registering `skerch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us get to some bonus content. One benefit of `laplax` is its modularity, which should make it easy to extend or bend its use cases. For example, we can easily register our favorite curvature approximation method: `skerch`; even though it was written for `torch`. To make it available for creating a posterior function based on its curvature structure, we can either implement (+register) all methods or refer to a default method, which might already exist in `laplax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from skerch import linops\n",
    "from skerch.decompositions import seigh\n",
    "\n",
    "from laplax.curv.utils import LowRankTerms, get_matvec\n",
    "from laplax.register import register_curvature_method\n",
    "from laplax.types import DType\n",
    "\n",
    "\n",
    "class JAXMV(linops.TorchLinOpWrapper):\n",
    "    def __init__(self, matvec, shape):\n",
    "        self.shape = shape\n",
    "        self.matvec = matvec\n",
    "\n",
    "    def __matmul__(self, x):\n",
    "        x_dtype = x.dtype\n",
    "        x = jnp.asarray(x.detach().cpu().numpy())\n",
    "        x = self.matvec(x)\n",
    "        return torch.tensor(np.asarray(x), dtype=x_dtype)\n",
    "\n",
    "    def __rmatmul__(self, x):\n",
    "        return self.__matmul__(x.T)\n",
    "\n",
    "\n",
    "def skerch_low_rank(\n",
    "    A,\n",
    "    *,\n",
    "    layout=None,\n",
    "    rank: int = 100,\n",
    "    return_dtype: DType = jnp.float64,\n",
    "    mv_jittable=True,\n",
    "    **kwargs,\n",
    "):\n",
    "    del kwargs\n",
    "    # Setup mv product.\n",
    "    matvec, size = get_matvec(A, layout=layout, jit=mv_jittable)\n",
    "    op = JAXMV(matvec, (size, size))\n",
    "\n",
    "    res = seigh(\n",
    "        op, op_device=\"cpu\", op_dtype=torch.float64, outer_dim=rank, inner_dim=rank\n",
    "    )\n",
    "\n",
    "    low_rank_result = LowRankTerms(\n",
    "        U=jnp.asarray((res[0] @ res[1]).detach().cpu()),\n",
    "        S=jnp.asarray(res[2].detach().cpu().numpy()),\n",
    "        scalar=jnp.asarray(0.0, dtype=return_dtype),\n",
    "    )\n",
    "    return low_rank_result\n",
    "\n",
    "\n",
    "register_curvature_method(\n",
    "    name=\"skerch\", create_curvature_fn=skerch_low_rank, default=\"lanczos\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from laplax.curv import create_posterior_fn\n",
    "\n",
    "posterior_fn = create_posterior_fn(\n",
    "    curv_type=\"skerch\",\n",
    "    mv=ggn_mv,\n",
    "    layout=params,\n",
    "    key=jax.random.key(20),\n",
    "    rank=50,\n",
    "    mv_jit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Posterior GP kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the posterior function to create a Laplace (GP) kernel to also model covariances between various inputs. While the default only takes a single input, we can use standard vectorization techniques to apply it to multiple inputs at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from laplax.eval.pushforward import set_posterior_gp_kernel\n",
    "\n",
    "gp_kernel, dist_state = set_posterior_gp_kernel(\n",
    "    model_fn=model_fn,\n",
    "    mean=params,\n",
    "    posterior_fn=posterior_fn,\n",
    "    prior_arguments=prior_arguments,\n",
    "    dense=True,  # If dense = False, then a slower kernel-vector product is returned.\n",
    "    output_layout=1,\n",
    ")\n",
    "\n",
    "\n",
    "def vectorized_laplace_kernel(a, b):\n",
    "    return jnp.vectorize(gp_kernel, signature=\"(d),(d)->(j,j)\")(a, b)[..., 0]\n",
    "\n",
    "\n",
    "X_pred = jnp.linspace(0.0, 8.0, 200).reshape(200, 1)\n",
    "Y_pred = model_fn(X_pred, params)[:, 0]\n",
    "Y_var = vectorized_laplace_kernel(X_pred, X_pred)\n",
    "\n",
    "_ = plot_regression_with_uncertainty(\n",
    "    X_train=train_batch[\"input\"],\n",
    "    y_train=train_batch[\"target\"],\n",
    "    X_pred=X_pred[:, 0],\n",
    "    y_pred=Y_pred,\n",
    "    y_std=jnp.sqrt(Y_var)[:, 0],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, in this little example this is not so much of an advantage. However, for more complex models/datasets this can be a huge support for downstream tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelOverfit(nnx.Module):\n",
    "    def __init__(self, rngs):\n",
    "        self.linear1 = nnx.Linear(1, 25, rngs=rngs)\n",
    "        self.linear2 = nnx.Linear(25, 50, rngs=rngs)\n",
    "        self.linear3 = nnx.Linear(50, 25, rngs=rngs)\n",
    "        self.linear4 = nnx.Linear(25, 1, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = nnx.tanh(self.linear1(x))\n",
    "        x = nnx.tanh(self.linear2(x))\n",
    "        x = nnx.tanh(self.linear3(x))\n",
    "        x = nnx.tanh(self.linear4(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class FullOverfit(nnx.Module):\n",
    "    def __init__(self, rngs):\n",
    "        self.linear1 = nnx.Linear(1, 25, rngs=rngs)\n",
    "        self.linear2 = nnx.Linear(25, 50, rngs=rngs)\n",
    "        self.linear3 = nnx.Linear(50, 25, rngs=rngs)\n",
    "        self.linear4 = nnx.Linear(25, 50, rngs=rngs)\n",
    "        self.linear5 = nnx.Linear(50, 25, rngs=rngs)\n",
    "        self.linear6 = nnx.Linear(25, 50, rngs=rngs)\n",
    "        self.linear7 = nnx.Linear(50, 25, rngs=rngs)\n",
    "        self.linear8 = nnx.Linear(25, 1, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = nnx.tanh(self.linear1(x))\n",
    "        x = nnx.tanh(self.linear2(x))\n",
    "        x = nnx.tanh(self.linear3(x))\n",
    "        x = nnx.tanh(self.linear4(x))\n",
    "        x = nnx.tanh(self.linear5(x))\n",
    "        x = nnx.tanh(self.linear6(x))\n",
    "        x = nnx.tanh(self.linear7(x))\n",
    "        x = nnx.tanh(self.linear8(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "n_epochs = 1000\n",
    "lr = 1e-3\n",
    "rngs = nnx.Rngs(0)\n",
    "models = [\n",
    "    (Model(1, 2, 1, rngs=rngs), 1000, 1e-3, \"1-2-1\"),\n",
    "    (Model(1, 50, 1, rngs=rngs), 1000, 1e-3, \"1-50-1\"),\n",
    "    (Model(1, 600, 1, rngs=rngs), 1000, 1e-3, \"1-600-1\"),\n",
    "    (ModelOverfit(rngs=rngs), n_epochs, 1e-3, \"1-25-50-25-1\"),\n",
    "    (FullOverfit(rngs=rngs), n_epochs, 1e-3, \"1-25-50-25-50-25-50-25-1\"),\n",
    "]\n",
    "\n",
    "trained_models = []\n",
    "for model, num_epoch, lr, name in models:\n",
    "    print(f\"Model {name}\")\n",
    "    model.name = name\n",
    "    trained_models.append(train_model(model, num_epoch, lr))\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marglik = {}\n",
    "train_batch = {\"input\": X_train, \"target\": y_train}\n",
    "prior_arguments = {\"prior_prec\": 1.0}\n",
    "curv_type = \"lanczos\"\n",
    "\n",
    "for model in trained_models:\n",
    "    # Prepare model\n",
    "    graph_def, params = nnx.split(model)\n",
    "\n",
    "    def model_fn(input, params):\n",
    "        return nnx.call((graph_def, params))(input)[0]  # noqa: B023\n",
    "\n",
    "    curv_approx = estimate_curvature(\n",
    "        curv_type=curv_type,\n",
    "        mv=create_ggn_mv(model_fn, params, train_batch, loss_fn=\"mse\"),\n",
    "        layout=params,\n",
    "        key=jax.random.key(0),  # If necessary\n",
    "        rank=50,  # If necessary\n",
    "    )\n",
    "\n",
    "    marglik[model.name] = marginal_log_likelihood(\n",
    "        curv_estimate=curv_approx,\n",
    "        prior_arguments=prior_arguments,\n",
    "        data=train_batch,\n",
    "        model_fn=model_fn,\n",
    "        params=params,\n",
    "        loss_fn=\"mse\",\n",
    "        curv_type=curv_type,\n",
    "    ).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(marglik, \"Marginal log-likelihood\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would choose the model with the highest marginal log-likelihood. If there exist additional (continuous/relaxed) model parameters, we could use again the marginal log-likelihood in a gradient-based optimization to find its *optimal* values. Such procedures are often discussed under the name of model selection and differentiable Laplace."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
