{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplace on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data and model and train it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start the tutorial by downloading and setting up a dataloader for `MNIST`. We will use a simple `flax.nnx` model for the training. The data + model setup and training closely follows the `flax.nnx` [documentation](https://flax.readthedocs.io/en/latest/mnist_tutorial.html) to stress the flexible post-hoc abilities of `laplax` (and, of course, Laplace Approximations in general)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Define constants\n",
    "train_steps = 2200\n",
    "eval_every = 200\n",
    "train_batch_size = 32\n",
    "val_batch_size = 32\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "])\n",
    "\n",
    "# Load MNIST datasets\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"data\", train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"data\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "def collate_fn(batch):\n",
    "    input, target = (\n",
    "        torch.stack([s[0] for s in batch]),\n",
    "        torch.tensor([s[1] for s in batch]),\n",
    "    )\n",
    "    return {\"input\": input.permute(0, 2, 3, 1).numpy(), \"target\": target.numpy()}\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=4,\n",
    "    prefetch_factor=2,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=val_batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=4,\n",
    "    prefetch_factor=2,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "\n",
    "# Create training iterator that yields for exactly train_steps\n",
    "train_iter = islice(train_loader, train_steps)\n",
    "num_training_samples = len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "from flax import nnx\n",
    "\n",
    "\n",
    "class CNN(nnx.Module):\n",
    "    \"\"\"A simple CNN model.\"\"\"\n",
    "\n",
    "    def __init__(self, *, rngs: nnx.Rngs):\n",
    "        self.conv1 = nnx.Conv(1, 32, kernel_size=(3, 3), padding=\"VALID\", rngs=rngs)\n",
    "        self.conv2 = nnx.Conv(32, 32, kernel_size=(3, 3), padding=\"VALID\", rngs=rngs)\n",
    "        self.conv3 = nnx.Conv(32, 64, kernel_size=(3, 3), padding=\"VALID\", rngs=rngs)\n",
    "        self.avg_pool1 = partial(nnx.avg_pool, window_shape=(2, 2), strides=(2, 2))\n",
    "        self.avg_pool2 = partial(nnx.avg_pool, window_shape=(3, 3), strides=(1, 1))\n",
    "        self.linear1 = nnx.Linear(64, 10, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.avg_pool1(nnx.relu(self.conv1(x)))\n",
    "        x = self.avg_pool1(nnx.relu(self.conv2(x)))\n",
    "        x = self.avg_pool2(nnx.relu(self.conv3(x)))\n",
    "        x = x.flatten()\n",
    "        x = self.linear1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "model = CNN(rngs=nnx.Rngs(0))\n",
    "\n",
    "\n",
    "# Create forward function with vmap\n",
    "@nnx.vmap(in_axes=(None, 0), out_axes=0)\n",
    "def forward(model: CNN, x):\n",
    "    return model(x)\n",
    "\n",
    "\n",
    "# Visualize it\n",
    "# nnx.display(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "learning_rate = 3e-4\n",
    "momentum = 0.9\n",
    "\n",
    "optimizer = nnx.Optimizer(model, optax.adamw(learning_rate, momentum))\n",
    "metrics = nnx.MultiMetric(\n",
    "    accuracy=nnx.metrics.Accuracy(),\n",
    "    loss=nnx.metrics.Average(\"loss\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup training step functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model: CNN, batch):\n",
    "    logits = forward(model, batch[\"input\"])\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "        logits=logits, labels=batch[\"target\"]\n",
    "    ).mean()\n",
    "    return loss, logits\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(model: CNN, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
    "    \"\"\"Train for a single step.\"\"\"\n",
    "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, logits), grads = grad_fn(model, batch)\n",
    "    metrics.update(loss=loss, logits=logits, labels=batch[\"target\"])  # In-place updates\n",
    "    optimizer.update(grads)  # In-place updates\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def eval_step(model: CNN, metrics: nnx.MultiMetric, batch):\n",
    "    loss, logits = loss_fn(model, batch)\n",
    "    metrics.update(loss=loss, logits=logits, labels=batch[\"target\"])  # In-place updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_history = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_accuracy\": [],\n",
    "    \"test_loss\": [],\n",
    "    \"test_accuracy\": [],\n",
    "}\n",
    "\n",
    "for step, batch in enumerate(train_iter):\n",
    "    # Run the optimization for one step and make a stateful update to the following:\n",
    "    # - The train state's model parameters\n",
    "    # - The optimizer state\n",
    "    # - The training loss and accuracy batch metrics\n",
    "    train_step(model, optimizer, metrics, batch)\n",
    "\n",
    "    if step > 0 and (step % eval_every == 0 or step == train_steps - 1):\n",
    "        # One training epoch has passed.\n",
    "        # Log the training metrics.\n",
    "        for metric, value in metrics.compute().items():  # Compute the metrics.\n",
    "            metrics_history[f\"train_{metric}\"].append(value)  # Record the metrics.\n",
    "        metrics.reset()  # Reset the metrics for the test set.\n",
    "\n",
    "        # Compute the metrics on the test set after each training epoch.\n",
    "        for test_batch in test_loader:\n",
    "            eval_step(model, metrics, test_batch)\n",
    "\n",
    "        # Log the test metrics.\n",
    "        for metric, value in metrics.compute().items():\n",
    "            metrics_history[f\"test_{metric}\"].append(value)\n",
    "        metrics.reset()  # Reset the metrics for the next training epoch.\n",
    "\n",
    "        print(\n",
    "            f\"[train] step: {step}, \"\n",
    "            f\"loss: {metrics_history['train_loss'][-1]}, \"\n",
    "            f\"accuracy: {metrics_history['train_accuracy'][-1] * 100}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"[test] step: {step}, \"\n",
    "            f\"loss: {metrics_history['test_loss'][-1]}, \"\n",
    "            f\"accuracy: {metrics_history['test_accuracy'][-1] * 100}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check model calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have followed along the standard MNIST tutorial from `flax.nnx`. Now, we want to check the calibration of the model, i.e. whether the probabilities it assigns to each class label represents its confidence. A good score for this is the ECE (see e.g. [Mucs√°nyi2023](https://trustworthyml.io/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import jax.numpy as jnp\n",
    "from plotting import create_proportion_diagram, create_reliability_diagram\n",
    "\n",
    "from laplax.eval.metrics import calculate_bin_metrics, correctness\n",
    "\n",
    "NUM_BINS = 15\n",
    "\n",
    "# Collect predictions and targets from test dataset\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "\n",
    "for batch in test_loader:\n",
    "    # Get predictions for this batch\n",
    "    predictions = jax.nn.softmax(forward(model, batch[\"input\"]), axis=1)\n",
    "    all_predictions.append(predictions)\n",
    "    all_targets.append(batch[\"target\"])\n",
    "\n",
    "\n",
    "# Concatenate all batches\n",
    "predictions = jnp.concatenate(all_predictions, axis=0)\n",
    "targets = jnp.concatenate(all_targets, axis=0)\n",
    "\n",
    "# Calculate confidence and correctness\n",
    "max_prob = predictions.max(axis=-1)\n",
    "correctness_float = correctness(pred=predictions, target=targets).astype(jnp.float32)\n",
    "\n",
    "print(f\"Accuracy: {correctness_float.mean():.4f}\")\n",
    "\n",
    "# Calculate bin metrics\n",
    "bin_proportions, bin_confidences, bin_accuracies = calculate_bin_metrics(\n",
    "    confidence=max_prob, correctness=correctness_float, num_bins=NUM_BINS\n",
    ")\n",
    "\n",
    "# Plot the reliability diagram\n",
    "create_reliability_diagram(\n",
    "    bin_confidences=bin_confidences,\n",
    "    bin_accuracies=bin_accuracies,\n",
    "    num_bins=NUM_BINS,\n",
    ")\n",
    "\n",
    "# Plot the proportion diagram\n",
    "create_proportion_diagram(\n",
    "    bin_proportions=bin_proportions,\n",
    "    num_bins=NUM_BINS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Laplace Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create the GGN matrix-vector product. As the GGN has 679,730,993,764 entries in this case, the naive representation of the dense matrix would take approximately 2.718 TB of VRAM. Therefore, it is crucial to represent this matrix-vector product _implicitly_, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from laplax.curv import create_ggn_mv\n",
    "\n",
    "# Create GGN\n",
    "graph_def, params = nnx.split(model)\n",
    "\n",
    "\n",
    "def model_fn(input, params):\n",
    "    return nnx.call((graph_def, params))(input)[0]\n",
    "\n",
    "\n",
    "train_batch = next(iter(train_loader))\n",
    "ggn_mv = create_ggn_mv(\n",
    "    model_fn,\n",
    "    params,\n",
    "    train_batch,\n",
    "    loss_fn=\"cross_entropy\",\n",
    "    num_total_samples=num_training_samples,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can consider the `ggn_mv` over a dataloader instead of a single training batch. To do so, the following cell needs to be commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from laplax.curv.ggn import create_ggn_mv_without_data\n",
    "from laplax.util.loader import DataLoaderMV, reduce_add\n",
    "\n",
    "\n",
    "# Set maximum number of batches\n",
    "class LimitedLoader:\n",
    "    \"\"\"DataLoader wrapper that limits the number of batches.\"\"\"\n",
    "\n",
    "    def __init__(self, loader, max_batches):\n",
    "        self.loader = loader\n",
    "        self.max_batches = max_batches\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch_iter = iter(self.loader)\n",
    "        for _ in range(self.max_batches):\n",
    "            yield next(batch_iter)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.max_batches\n",
    "\n",
    "\n",
    "NUM_OF_BATCHES = 4\n",
    "train_loader_limited = LimitedLoader(train_loader, NUM_OF_BATCHES)\n",
    "\n",
    "# Setup batch-wise GGN-matrix-vector product\n",
    "ggn_mv_wo_data = create_ggn_mv_without_data(\n",
    "    model_fn,\n",
    "    params,\n",
    "    loss_fn=\"cross_entropy\",\n",
    "    factor=num_training_samples / (train_batch_size * NUM_OF_BATCHES),\n",
    ")\n",
    "\n",
    "# Setup ggn_mv with DataLoader\n",
    "ggn_mv = DataLoaderMV(\n",
    "    ggn_mv_wo_data,\n",
    "    train_loader_limited,\n",
    "    transform=lambda x: x,\n",
    "    reduce=reduce_add,\n",
    "    verbose_logging=True,  # Shows progress bar when iterating through data loader.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callables such as `to_dense`, `diagonal` and `wrap_function` are lowered into the sum of the `DataLoaderMV`. The cell below can be executed if the model above is made sufficiently small such that the GGN fits into memory. \n",
    "\n",
    "**Note:** For the remainder of the notebook, we assume an un-wrapped mv function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from laplax.util.flatten import create_pytree_flattener, wrap_function\n",
    "# from laplax.util.mv import to_dense\n",
    "# from laplax.util.tree import get_size\n",
    "\n",
    "# flatten, unflatten = create_pytree_flattener(params)\n",
    "# ggn_mv = wrap_function(ggn_mv, input_fn=unflatten, output_fn=flatten)\n",
    "# arr = to_dense(ggn_mv, layout=get_size(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the GGN matrix-vector product above to obtain a low-rank approximation of the GGN. Even though the dense GGN cannot be represented in memory, its low-rank approximation for a sufficiently low rank remains tractable to hold in memory. Having access to the low-rank GGN terms, we can then efficiently invert an isotropically dampened version of it which is the weight-space covariance matrix of our Laplace approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from laplax.curv.cov import create_posterior_fn\n",
    "\n",
    "# Create Posterior\n",
    "posterior_fn = create_posterior_fn(\n",
    "    \"lanczos\",\n",
    "    mv=ggn_mv,\n",
    "    layout=params,\n",
    "    key=jax.random.key(0),\n",
    "    maxiter=100,\n",
    "    mv_jit=True,  # If the loader has side effects, such as i/o operations, then\n",
    "    # this should be set to False.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need a way to represent the model's uncertainty in its output space, as decisions (such as abstaining from prediction) are made based on the model output, not on the weight space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from laplax.eval.metrics import expected_calibration_error\n",
    "from laplax.eval.pushforward import (\n",
    "    lin_mc_pred_act,\n",
    "    lin_pred_mean,\n",
    "    lin_setup,\n",
    "    set_lin_pushforward,\n",
    ")\n",
    "\n",
    "prior_arguments = {\"prior_prec\": 10000.0}\n",
    "pushforward_fns = [\n",
    "    lin_setup,\n",
    "    lin_pred_mean,\n",
    "    lin_mc_pred_act,\n",
    "]\n",
    "\n",
    "pushforward_fn = set_lin_pushforward(\n",
    "    model_fn=model_fn,\n",
    "    mean_params=params,\n",
    "    posterior_fn=posterior_fn,\n",
    "    prior_arguments=prior_arguments,\n",
    "    pushforward_fns=pushforward_fns,\n",
    "    key=jax.random.key(0),\n",
    "    num_samples=30,\n",
    ")\n",
    "\n",
    "# Set up two versions of the pushforward function - with and without vmap.\n",
    "pushforward_fn_jit = jax.jit(pushforward_fn)\n",
    "pushforward_fn_jit_vmap = jax.jit(jax.vmap(pushforward_fn))\n",
    "\n",
    "\n",
    "# Define the metrics function (ideally with batch dimension)\n",
    "def confidences_map(map_, **kwargs):\n",
    "    del kwargs\n",
    "    return jnp.max(jax.nn.softmax(map_, axis=-1), axis=-1)\n",
    "\n",
    "\n",
    "def confidences_pred(pred_act, **kwargs):\n",
    "    del kwargs\n",
    "    return jnp.max(pred_act, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from laplax.eval import apply_fns, evaluate_metrics_on_generator, transfer_entry\n",
    "\n",
    "results = evaluate_metrics_on_generator(\n",
    "    pushforward_fn_jit,\n",
    "    test_loader,\n",
    "    metrics=[\n",
    "        transfer_entry([\"pred_mean\", \"map\", \"mc_pred_act\"]),\n",
    "        apply_fns(\n",
    "            confidences_map,\n",
    "            confidences_pred,\n",
    "            correctness,\n",
    "            names=[\"confidences_map\", \"confidences_pred\", \"correctness_map\"],\n",
    "            map_=\"map\",\n",
    "            pred_act=\"mc_pred_act\",\n",
    "            pred=\"map\",\n",
    "            target=\"target\",\n",
    "        ),\n",
    "        apply_fns(\n",
    "            correctness,\n",
    "            names=[\"correctness_pred\"],\n",
    "            pred=\"mc_pred_act\",\n",
    "            target=\"target\",\n",
    "        ),\n",
    "    ],\n",
    "    reduce=jnp.concatenate,\n",
    "    vmap_over_data=True,\n",
    ")\n",
    "\n",
    "confidences_map_val = results[\"confidences_map\"].astype(jnp.float32)\n",
    "correctness_map_val = results[\"correctness_map\"].astype(jnp.float32)\n",
    "confidences_pred_val = results[\"confidences_pred\"].astype(jnp.float32)\n",
    "correctness_pred_val = results[\"correctness_pred\"].astype(jnp.float32)\n",
    "\n",
    "ece_map = expected_calibration_error(\n",
    "    confidence=confidences_map_val, correctness=correctness_map_val, num_bins=NUM_BINS\n",
    ")\n",
    "ece_pred = expected_calibration_error(\n",
    "    confidence=confidences_pred_val, correctness=correctness_pred_val, num_bins=NUM_BINS\n",
    ")\n",
    "\n",
    "print(f\"MAP ECE: {ece_map:.4f}\")\n",
    "print(f\"MAP acc: {correctness_map_val.mean():.4f}\")\n",
    "print(f\"Laplace ECE: {ece_pred:.4f}\")\n",
    "print(f\"Laplace acc: {correctness_pred_val.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all functions (pushforward and metrics) support a batch dimension, then we can set `has_batch` to `False`. Otherwise, `has_batch=True` will apply `jax.lax.map` along the batch dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from laplax.eval import apply_fns, transfer_entry\n",
    "\n",
    "results = evaluate_metrics_on_generator(\n",
    "    pushforward_fn_jit_vmap,\n",
    "    test_loader,\n",
    "    metrics=[\n",
    "        transfer_entry([\"pred_mean\", \"map\", \"mc_pred_act\"]),\n",
    "        apply_fns(\n",
    "            confidences_map,\n",
    "            confidences_pred,\n",
    "            correctness,\n",
    "            # correctness_pred_act,\n",
    "            names=[\n",
    "                \"confidences_map\",\n",
    "                \"confidences_pred\",\n",
    "                \"correctness_map\",\n",
    "                # \"correctness_pred\"\n",
    "            ],\n",
    "            map_=\"map\",\n",
    "            pred_act=\"mc_pred_act\",\n",
    "            pred=\"map\",\n",
    "            target=\"target\",\n",
    "        ),\n",
    "        apply_fns(\n",
    "            correctness,\n",
    "            names=[\"correctness_pred\"],\n",
    "            pred=\"mc_pred_act\",\n",
    "            target=\"target\",\n",
    "        ),\n",
    "    ],\n",
    "    reduce=jnp.concatenate,\n",
    "    vmap_over_data=False,\n",
    "    # If all functions handle batch dimensions properly, then setting false works.\n",
    ")\n",
    "\n",
    "confidences_map_val = results[\"confidences_map\"].astype(jnp.float32)\n",
    "correctness_map_val = results[\"correctness_map\"].astype(jnp.float32)\n",
    "confidences_pred_val = results[\"confidences_pred\"].astype(jnp.float32)\n",
    "correctness_pred_val = results[\"correctness_pred\"].astype(jnp.float32)\n",
    "\n",
    "ece_map = expected_calibration_error(\n",
    "    confidence=confidences_map_val, correctness=correctness_map_val, num_bins=NUM_BINS\n",
    ")\n",
    "ece_pred = expected_calibration_error(\n",
    "    confidence=confidences_pred_val, correctness=correctness_pred_val, num_bins=NUM_BINS\n",
    ")\n",
    "\n",
    "print(f\"MAP ECE: {ece_map:.4f}\")\n",
    "print(f\"MAP acc: {correctness_map_val.mean():.4f}\")\n",
    "print(f\"Laplace ECE: {ece_pred:.4f}\")\n",
    "print(f\"Laplace acc: {correctness_pred_val.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
