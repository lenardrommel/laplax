# calibrate.py

"""Calibration utilities for optimizing prior precision in probabilistic models.

This script provides utilities for optimizing prior precision in probabilistic models.
It includes functions to:

- Evaluate metrics for given prior arguments and datasets.
- Perform grid search to optimize prior precision using objective functions.
- Optimize prior precision over a logarithmic grid interval.

The script leverages JAX for numerical operations, Loguru for logging, and custom
utilities from the `laplax` package.
"""

import time
from collections.abc import Callable

import jax
import jax.numpy as jnp
import numpy as np
import optax
from jax import grad, vmap
from loguru import logger

from laplax.eval.metrics import chi_squared_zero
from laplax.types import Array, Data, Float, Kwargs, PriorArguments


def evaluate_for_given_prior_arguments(
    *,
    data: Data,
    set_prob_predictive: Callable,
    metric: Callable = chi_squared_zero,
    **kwargs: Kwargs,
) -> Float:
    """Evaluate the metric for a given set of prior arguments and data.

    This function computes predictions for the input data using a probabilistic
    predictive function generated by `set_prob_predictive`. It then evaluates a
    specified metric using these predictions.

    Args:
        data: Dataset containing inputs and targets.
        set_prob_predictive: A callable that generates a probabilistic predictive
            function.
        metric: A callable metric function to evaluate the predictions
            (default: `calibration_metric`).
        **kwargs: Additional arguments passed to `set_prob_predictive` and
            `jax.lax.map`.

    Returns:
        The evaluated metric value.
    """
    prob_predictive = set_prob_predictive(**kwargs)

    def evaluate_data(dp: Data) -> dict[str, Array]:
        return {**prob_predictive(dp["input"]), "target": dp["target"]}

    res = metric(
        **jax.lax.map(
            evaluate_data,
            data,
            batch_size=kwargs.get(
                "evaluate_for_given_prior_arguments_batch_size",
                kwargs.get("data_batch_size"),
            ),
        )
    )
    return res


def grid_search(
    prior_prec_interval: Array,
    objective: Callable[[PriorArguments], float],
    patience: int | None = None,
    max_iterations: int | None = None,
) -> Float:
    """Perform grid search to optimize prior precision.

    This function iteratively evaluates an objective function over a range of
    prior precisions. It tracks the performance and stops early if results
    increase consecutively for a specified number of iterations (`patience`).
    The prior precision which scores the lowest is returned.

    Args:
        prior_prec_interval: An array of prior precision values to search.
        objective: A callable objective function that takes `PriorArguments` as input
            and returns a float result.
        patience: The number of consecutive iterations with increasing results to
            tolerate before stopping (default: 5).
        max_iterations: The maximum number of iterations to perform (default: None).

    Returns:
        The prior precision value that minimizes the objective function.
    """
    results, prior_precs = [], []
    increasing_count = 0
    previous_result = None

    for iteration, prior_prec in enumerate(prior_prec_interval):
        start_time = time.perf_counter()
        try:
            result = objective({"prior_prec": prior_prec})
        except ValueError as error:
            logger.warning(f"Caught an exception in validate {error}")
            result = float("inf")

        if jnp.isnan(result):
            logger.info("Caught nan, setting result to inf.")
            result = float("inf")

        # Logging for performance and tracking
        logger.info(
            f"Took {time.perf_counter() - start_time:.4f} seconds, "
            f"prior prec: {prior_prec:.4f}, "
            f"result: {result:.6f}",
        )

        results.append(result)
        prior_precs.append(prior_prec)

        # If we have a previous result, check if the result has increased
        if patience is not None and previous_result is not None:
            if result > previous_result:
                increasing_count += 1
                logger.info(f"Result increased, increasing_count = {increasing_count}")
            else:
                increasing_count = 0

            # Stop if the results have increased for `patience` consecutive iterations
            if increasing_count >= patience:
                break

        previous_result = result

        # Check if maximum iterations reached
        if max_iterations is not None and iteration >= max_iterations:
            logger.info(f"Stopping due to reaching max iterations = {max_iterations}")
            break

    best_prior_prec = prior_precs[np.nanargmin(results)]
    logger.info(f"Chosen prior prec = {best_prior_prec:.4f}")

    return best_prior_prec


def adam(
    objective: Callable[[PriorArguments], float],
    initial_log_prior_prec: float,
    learning_rate: float = 1e-1,
    max_iter: int | None = 100,
    patience: int | None = None,
    tol: float | None = None,
    bounds: tuple[float, float] | None = None,
    **kwargs: Kwargs,
) -> Float:
    """Perform adam to optimize prior precision.

    This function uses the Adam optimizer to find the optimal prior precision by minimizing
    an objective function. The optimization is performed in log-space for numerical stability.
    The function includes early stopping mechanisms based on gradient tolerance and patience
    for consecutive loss increases.
    Args:
        prior_prec_interval: An array of prior precision values to search.
        objective: A callable objective function that takes `PriorArguments` as input
            and returns a float result.
        patience: The number of consecutive iterations with increasing results to
            tolerate before stopping (default: 5).
        max_iterations: The maximum number of iterations to perform (default: None).

    Returns:
        The prior precision value that minimizes the objective function.
    """
    clip_grad = kwargs.get("clip_grad", 1.0)
    optimizer = optax.chain(
        optax.clip_by_global_norm(clip_grad), optax.adam(learning_rate)
    )

    param = jnp.array(initial_log_prior_prec)
    opt_state = optimizer.init(param)

    history = {"loss": [], "prior_prec": [], "log_prior_prec": [], "grad_norm": []}
    value_and_grad_fn = jax.value_and_grad(objective)

    best_loss = float("inf")
    best_param = param
    previous_loss: float | None = None
    increasing_count = 0

    for i in range(max_iter or 1000):
        prior_prec = jnp.exp(param)

        try:
            loss, grad = value_and_grad_fn({"prior_prec": prior_prec})
        except ValueError as error:
            logger.warning(f"Caught an exception in validate {error}")
            loss = jnp.inf
            grad = {"prior_prec": jnp.array(0.0, dtype=param.dtype)}

        if jnp.isnan(loss):
            logger.info("Caught nan, setting result to inf.")
            loss = jnp.inf

        grad_prior = grad["prior_prec"]
        grad_log_prior = grad_prior * prior_prec

        if not jnp.isfinite(loss) or not jnp.isfinite(grad_log_prior):
            logger.warning(f"NaN/Inf detected at iteration {i}, stopping early")
            break

        if loss < best_loss:
            best_loss = float(loss)
            best_param = param

        grad_norm = float(jnp.abs(grad_log_prior))

        history["loss"].append(float(loss))
        history["prior_prec"].append(float(prior_prec))
        history["log_prior_prec"].append(float(param))
        history["grad_norm"].append(grad_norm)

        if i % 10 == 0:
            logger.debug(
                f"Iter {i:3d}: loss={loss:.4e}, prior_prec={prior_prec:.4e}, "
                f"|grad|={grad_norm:.4e}"
            )

        if tol is not None and grad_norm < tol:
            logger.info(f"Converged at iteration {i} (|grad|={grad_norm:.4e})")
            break

        if patience is not None and previous_loss is not None:
            if loss > previous_loss:
                increasing_count += 1
                logger.debug(f"Loss increased; increasing_count={increasing_count}")
            else:
                increasing_count = 0

            if increasing_count >= patience:
                logger.info(
                    f"Stopping after {increasing_count} consecutive loss increases"
                )
                break

        previous_loss = float(loss)

        updates, opt_state = optimizer.update(grad_log_prior, opt_state, param)
        param = optax.apply_updates(param, updates)

        if bounds is not None:
            param = jnp.clip(param, bounds[0], bounds[1])

    best_prior_prec = float(jnp.exp(best_param))
    return best_prior_prec


def adam_with_restarts(
    objective: Callable[[PriorArguments], float],
    initial_log_prior_prec: float,
    restarts: int,
    **kwargs: Kwargs,
) -> Float:
    """Perform adam optimization with random restarts to find global optimum.

    This function runs the Adam optimizer multiple times from different random
    initial points and returns the best result across all restarts. This helps
    avoid local minima in the optimization landscape.

    Args:
        objective: A callable objective function that takes `PriorArguments`
            as input and returns a float result.
        initial_log_prior_prec: The initial log prior precision for the first
            restart.
        restarts: Number of random restarts to perform (including the initial
            point).
        **kwargs: Additional arguments passed to `adam`, may include:
            - bounds: tuple[float, float] for log space bounds
            - learning_rate: float
            - max_iter: int
            - patience: int
            - tol: float

    Returns:
        The prior precision value that minimizes the objective function across
        all restarts.
    """
    bounds = kwargs.get("bounds")
    rng = np.random.default_rng()

    best_prior_prec = None
    best_loss = float("inf")

    for restart_idx in range(restarts):
        if restart_idx == 0:
            init_log_prior_prec = initial_log_prior_prec
        elif bounds is not None:
            init_log_prior_prec = float(rng.uniform(bounds[0], bounds[1]))
        else:
            init_log_prior_prec = float(rng.normal(initial_log_prior_prec, 2.0))

        logger.info(
            f"Restart {restart_idx + 1}/{restarts}: "
            f"initial_log_prior_prec={init_log_prior_prec:.4f}"
        )

        prior_prec = adam(
            objective,
            init_log_prior_prec,
            **kwargs,
        )

        try:
            loss = objective({"prior_prec": prior_prec})
        except ValueError as error:
            logger.warning(f"Caught an exception in restart {restart_idx}: {error}")
            loss = float("inf")

        if jnp.isnan(loss):
            loss = float("inf")

        logger.info(
            f"Restart {restart_idx + 1}/{restarts} completed: "
            f"prior_prec={prior_prec:.4f}, loss={loss:.6f}"
        )

        if loss < best_loss:
            best_loss = loss
            best_prior_prec = prior_prec
            logger.info(f"New best found at restart {restart_idx + 1}")

    logger.info(
        f"Best across {restarts} restarts: "
        f"prior_prec={best_prior_prec:.4f}, loss={best_loss:.6f}"
    )

    return best_prior_prec


def stein_variational_gradient_descent(
    objective: Callable[[PriorArguments], float],
    initial_log_prior_prec: float,
    num_particles: int = 2,
    learning_rate: float = 1e-2,
    max_iter: int | None = None,
    **kwargs: Kwargs,
) -> Float:
    """Perform Stein Variational Gradient Descent to optimize prior precision.

    This function uses the Stein Variational Gradient Descent (SVGD)
    https://arxiv.org/abs/1608.04471 algorithm to find the optimal prior
    precision by minimizing an objective function. The optimization is performed
    in log-space for numerical stability.

    Args:
        objective: A callable objective function that takes `PriorArguments`
            as input and returns a float result.
        initial_log_prior_prec: The initial log prior precision.
        num_particles: The number of particles to use in SVGD (default: 10).
        learning_rate: The learning rate for the optimizer (default: 1e-2).
        max_iter: The maximum number of iterations to perform (default: None).
        **kwargs: Additional arguments passed to the SVGD optimizer, may include:
            - patience: int for early stopping
            - tol: float for gradient tolerance
            - bounds: tuple[float, float] for log space bounds
            - clip_grad: float for gradient clipping
            - bandwidth: float for kernel bandwidth (-1 for median heuristic)
    Returns:
        The prior precision value that minimizes the objective function.
    """
    clip_grad = kwargs.get("clip_grad", 1.0)
    patience = kwargs.get("patience")
    tol = kwargs.get("tol")
    bounds = kwargs.get("bounds")
    bandwidth = kwargs.get("bandwidth", -1.0)
    rng = np.random.default_rng()

    if bounds is not None:
        particles = jnp.array(rng.uniform(bounds[0], bounds[1], size=(num_particles,)))
    else:
        particles = jnp.array(
            rng.normal(initial_log_prior_prec, 1.0, size=(num_particles,))
        )

    # Setup optimizer (using Adagrad as in the original SVGD paper)
    optimizer = optax.chain(
        optax.clip_by_global_norm(clip_grad),
        optax.adagrad(learning_rate),
    )
    opt_state = optimizer.init(particles)

    grad_fn = jax.grad(lambda prec: objective({"prior_prec": prec}))

    def rbf_kernel(x: Array, y: Array, h: float) -> Float:
        return jnp.exp(-((x - y) ** 2) / (2 * h**2))

    def compute_median_bandwidth(particles: Array) -> Float:
        diff = particles[:, None] - particles[None, :]
        sq_dist = diff**2
        median_sq_dist = jnp.median(sq_dist[sq_dist > 0])
        return jnp.sqrt(median_sq_dist / jnp.log(num_particles + 1))

    def svgd_gradient(particles: Array, h: float) -> Array:
        def safe_grad(log_prec: Float) -> Float:
            prec = jnp.exp(log_prec)
            grad_val = grad_fn(prec)
            return -grad_val * prec

        log_prob_grads = vmap(safe_grad)(particles)

        def kernel_and_grad(x: Array) -> tuple[Array, Array]:
            k = vmap(lambda y: rbf_kernel(x, y, h))(particles)
            grad_k = vmap(lambda y: grad(rbf_kernel, argnums=0)(x, y, h))(particles)
            return k, grad_k

        kxy, grad_kxy = vmap(kernel_and_grad)(particles)

        # SVGD update: (1/n) * sum_j [k(x_j, x_i) * grad_log_p(x_j) + grad_k(x_j, x_i)]
        svgd_grad = (
            jnp.matmul(kxy, log_prob_grads) + jnp.sum(grad_kxy, axis=1)
        ) / num_particles

        return svgd_grad

    best_loss = float("inf")
    best_particle = particles[0]
    previous_mean_loss: float | None = None
    increasing_count = 0

    for iteration in range(max_iter or 1000):
        h = compute_median_bandwidth(particles) if bandwidth <= 0 else bandwidth

        prior_precs = jnp.exp(particles)
        losses = []
        for prec in prior_precs:
            try:
                loss = objective({"prior_prec": float(prec)})
                if jnp.isnan(loss):
                    loss = float("inf")
            except ValueError:
                loss = float("inf")
            losses.append(loss)

        losses = jnp.array(losses)
        mean_loss = float(jnp.mean(losses))
        min_loss = float(jnp.min(losses))

        if min_loss < best_loss:
            best_loss = min_loss
            best_particle = particles[jnp.argmin(losses)]

        svgd_grad = svgd_gradient(particles, h)
        grad_norm = float(jnp.linalg.norm(svgd_grad))

        if iteration % 10 == 0:
            logger.debug(
                f"Iter {iteration:3d}: mean_loss={mean_loss:.4e}, "
                f"best_loss={min_loss:.4e}, |grad|={grad_norm:.4e}, h={h:.4f}"
            )

        if tol is not None and grad_norm < tol:
            logger.info(f"Converged at iteration {iteration} (|grad|={grad_norm:.4e})")
            break

        if patience is not None and previous_mean_loss is not None:
            if mean_loss > previous_mean_loss:
                increasing_count += 1
                logger.debug(
                    f"Mean loss increased; increasing_count={increasing_count}"
                )
            else:
                increasing_count = 0

            if increasing_count >= patience:
                logger.info(
                    f"Stopping after {increasing_count} consecutive mean loss increases"
                )
                break

        previous_mean_loss = mean_loss

        updates, opt_state = optimizer.update(svgd_grad, opt_state, particles)
        particles = optax.apply_updates(particles, updates)

        if bounds is not None:
            particles = jnp.clip(particles, bounds[0], bounds[1])

    best_prior_prec = float(jnp.exp(best_particle))
    logger.info(
        f"SVGD completed: best_prior_prec={best_prior_prec:.4f}, "
        f"best_loss={best_loss:.6f}"
    )

    return best_prior_prec


def optimize_prior_prec_with_grid_search(
    objective: Callable[[PriorArguments], float],
    log_prior_prec_min: float = -5.0,
    log_prior_prec_max: float = 6.0,
    grid_size: int = 300,
    **kwargs: Kwargs,
) -> Float:
    """Optimize prior precision using logarithmic grid search.

    This function creates a logarithmically spaced interval of prior precision
    values and performs a grid search to find the optimal value that minimizes
    the specified objective function.

    Args:
        objective: A callable objective function that takes `PriorArguments` as input
            and returns a float result.
        log_prior_prec_min: The base-10 logarithm of the minimum prior precision
            value (default: -5.0).
        log_prior_prec_max: The base-10 logarithm of the maximum prior precision
            value (default: 6.0).
        grid_size: The number of points in the grid interval (default: 300).
        **kwargs: Additional arguments passed to `grid_search`.

    Returns:
        The optimized prior precision value.
    """
    prior_prec_interval = jnp.logspace(
        start=log_prior_prec_min,
        stop=log_prior_prec_max,
        num=grid_size,
    )
    prior_prec = grid_search(
        prior_prec_interval,
        objective,
        **kwargs,
    )

    return prior_prec


def optimize_prior_prec_stochastic(
    objective: Callable[[PriorArguments], float],
    initial_log_prior_prec: float = 0.0,
    method: str = "adam",
    **kwargs: Kwargs,
) -> Float:
    """Optimize prior precision using stochastic optimization methods.

    This function provides a unified interface for stochastic optimization
    methods including SVGD (default), Adam, and Adam with restarts.

    Args:
        objective: A callable objective function that takes `PriorArguments`
            as input and returns a float result.
        initial_log_prior_prec: The initial log prior precision (default: 0.0).
        method: Optimization method to use: "svgd", "adam", or "adam_restarts"
            (default: "svgd").
        **kwargs: Additional arguments passed to the optimization method:
            For SVGD:
                - num_particles: int (default: 10)
                - bandwidth: float (default: -1.0 for median heuristic)
            For Adam:
                - No special arguments
            For Adam with restarts:
                - restarts: int (default: 5)
            Common arguments:
                - learning_rate: float (default: 1e-2)
                - max_iter: int (default: 1000)
                - patience: int
                - tol: float
                - bounds: tuple[float, float]

    Returns:
        The optimized prior precision value.

    Raises:
        ValueError: If an unknown optimization method is specified.
    """
    if method == "svgd":
        prior_prec = stein_variational_gradient_descent(
            objective,
            initial_log_prior_prec,
            **kwargs,
        )
    elif method == "adam":
        prior_prec = adam(
            objective,
            initial_log_prior_prec,
            **kwargs,
        )
    elif method == "adam_restarts":
        restarts = kwargs.pop("restarts", 5)
        prior_prec = adam_with_restarts(
            objective,
            initial_log_prior_prec,
            restarts,
            **kwargs,
        )
    else:
        msg = f"Unknown optimization method: {method}"
        raise ValueError(msg)

    return prior_prec


def optimize_prior_prec(
    objective: Callable[[PriorArguments], float],
    log_prior_prec_min: float = -5.0,
    log_prior_prec_max: float = 6.0,
    grid_size: int = 300,
    stochastic: bool = False,
    **kwargs: Kwargs,
) -> Float:
    """Optimize prior precision using grid search or stochastic optimization.

    This function provides a unified interface for optimizing prior precision.
    By default, it uses grid search. When stochastic=True, it uses SVGD
    (Stein Variational Gradient Descent) by default, but can use Adam or
    Adam with restarts if specified via the 'method' kwarg.

    Args:
        objective: A callable objective function that takes `PriorArguments`
            as input and returns a float result.
        log_prior_prec_min: The base-10 logarithm of the minimum prior
            precision value (default: -5.0).
        log_prior_prec_max: The base-10 logarithm of the maximum prior
            precision value (default: 6.0).
        grid_size: The number of points in the grid interval (default: 300).
        stochastic: If False, use grid search; if True, use stochastic
            optimization (default: False).
        **kwargs: Additional arguments:
            - For grid search: patience, max_iterations
            - For stochastic optimization:
                - method: "svgd" (default), "adam", or "adam_restarts"
                - num_particles: int (for SVGD, default: 10)
                - restarts: int (for adam_restarts, default: 5)
                - learning_rate: float (default: 1e-2)
                - max_iter: int (default: 1000)
                - patience: int
                - tol: float
                - bounds: tuple[float, float] (optional log space bounds)

    Returns:
        The optimized prior precision value.
    """
    if not stochastic:
        prior_prec = optimize_prior_prec_with_grid_search(
            objective,
            log_prior_prec_min,
            log_prior_prec_max,
            grid_size,
            **kwargs,
        )
    else:
        initial_log_prior_prec = (log_prior_prec_min + log_prior_prec_max) / 2.0
        # Set bounds if not provided
        if "bounds" not in kwargs:
            kwargs["bounds"] = (log_prior_prec_min, log_prior_prec_max)
        prior_prec = optimize_prior_prec_stochastic(
            objective,
            initial_log_prior_prec,
            **kwargs,
        )

    return prior_prec


def optimize_with_adam(
    objective: Callable,
    initial_log_prior_prec: float,
    initial_log_sigma: float | None = None,
    learning_rate: float = 1e-2,
    max_iter: int = 100,
    tol: float = 1e-8,
    clip_grad: float = 1.0,
    bounds: tuple[float, float] | None = None,
    sigma_bounds: tuple[float, float] | None = None,
    patience: int | None = None,
) -> tuple[dict[str, float], float, dict]:
    optimizer = optax.chain(
        optax.clip_by_global_norm(clip_grad), optax.adam(learning_rate)
    )

    # Initialize parameters
    params = jnp.array([initial_log_prior_prec, initial_log_sigma])

    opt_state = optimizer.init(params)

    # Initialize history
    history = {
        "loss": [],
        "prior_prec": [],
        "log_prior_prec": [],
        "grad_norm": [],
    }
    history["sigma"] = []
    history["log_sigma"] = []

    value_and_grad_fn = jax.value_and_grad(objective)

    best_loss = float("inf")
    best_params = params
    previous_loss: float | None = None
    increasing_count = 0

    for i in range(max_iter):
        prior_prec = jnp.exp(params[0])

        # Build objective arguments
        obj_args = {"prior_prec": prior_prec}
        sigma = jnp.exp(params[1])
        obj_args["sigma"] = sigma

        try:
            loss, grad = value_and_grad_fn(obj_args)
        except Exception as error:  # noqa: BLE001
            logger.warning(f"Caught an exception in validate {error}")
            loss = jnp.inf
            grad = {k: jnp.array(0.0, dtype=params.dtype) for k in obj_args.keys()}

        if jnp.isnan(loss):
            logger.info("Caught nan, setting result to inf.")
            loss = jnp.inf

        # Compute gradients in log-space (chain rule)
        grad_prior = grad["prior_prec"]
        grad_log_prior = grad_prior * prior_prec

        grad_sigma = grad["sigma"]
        grad_log_sigma = grad_sigma * sigma
        grad_log_params = jnp.array([grad_log_prior, grad_log_sigma])

        # Check for NaN/Inf
        if not jnp.isfinite(loss) or not jnp.all(jnp.isfinite(grad_log_params)):
            logger.warning(f"NaN/Inf detected at iteration {i}, stopping early")
            break

        # Track best result
        if loss < best_loss:
            best_loss = float(loss)
            best_params = params

        # Compute gradient norm
        grad_norm = float(jnp.linalg.norm(grad_log_params))

        # Track history
        history["loss"].append(float(loss))
        history["prior_prec"].append(float(prior_prec))
        history["log_prior_prec"].append(float(params[0]))
        history["sigma"].append(float(sigma))
        history["log_sigma"].append(float(params[1]))
        history["grad_norm"].append(grad_norm)

        # Log progress
        if i % 10 == 0:
            logger.debug(
                f"Iter {i:3d}: loss={loss:.4e}, prior_prec={prior_prec:.4e}, "
                f"sigma={sigma:.4e}, |grad|={grad_norm:.4e}"
            )

        # Check convergence
        if grad_norm < tol:
            logger.info(f"Converged at iteration {i} (|grad|={grad_norm:.4e})")
            break

        # Early stopping on consecutive increases to mirror laplax patience
        if patience is not None and previous_loss is not None:
            if loss > previous_loss:
                increasing_count += 1
                logger.debug(f"Loss increased; increasing_count={increasing_count}")
            else:
                increasing_count = 0

            if increasing_count >= patience:
                logger.info(
                    f"Stopping after {increasing_count} consecutive loss increases"
                )
                break

        previous_loss = float(loss)

        # Update parameters (perform chain rule in log-space)
        updates, opt_state = optimizer.update(grad_log_params, opt_state, params)
        params = optax.apply_updates(params, updates)

        # Apply bounds if specified
        if bounds is not None:
            params = params.at[0].set(jnp.clip(params[0], bounds[0], bounds[1]))
        if sigma_bounds is not None:
            params = params.at[1].set(
                jnp.clip(params[1], sigma_bounds[0], sigma_bounds[1])
            )

    # Convert best parameters to dictionary
    result = {"prior_prec": float(jnp.exp(best_params[0]))}
    sigma = float(jnp.exp(best_params[1]))
    result["sigma"] = sigma
    result["sigma_squared"] = float(sigma**2)

    return result, float(best_loss), history


def optimize_prior_and_sigma(
    objective: Callable,
    log_prior_prec_min: float = -5.0,
    log_prior_prec_max: float = 6.0,
    log_sigma_min: float = -5.0,
    log_sigma_max: float = 6.0,
    grid_size: int = 300,
    **kwargs,
):
    bounds = (log_prior_prec_min, log_prior_prec_max)
    sigma_bounds = (log_sigma_min, log_sigma_max)

    key = kwargs.get("rng") or kwargs.get("key") or jax.random.PRNGKey(12)
    key, subkey1, subkey2 = jax.random.split(key, 3)

    # Initialize prior precision
    init_log_prior_prec = jax.random.uniform(
        subkey1, minval=bounds[0], maxval=bounds[1]
    )

    # Initialize noise precision if needed
    init_log_sigma = jax.random.uniform(
        subkey2, minval=sigma_bounds[0], maxval=sigma_bounds[1]
    )

    patience = kwargs.get("patience")
    learning_rate = kwargs.get("learning_rate", 1e-1)
    tol = kwargs.get("tol", 1e-8)
    clip_grad = kwargs.get("clip_grad", 1.0)
    max_iter = kwargs.get("max_iter", kwargs.get("max_iterations", grid_size))
    if max_iter is None:
        max_iter = grid_size

    result, _, _ = optimize_with_adam(
        objective=objective,
        initial_log_prior_prec=float(init_log_prior_prec),
        initial_log_sigma=float(init_log_sigma) if init_log_sigma is not None else None,
        learning_rate=learning_rate,
        max_iter=int(max_iter),
        tol=tol,
        clip_grad=clip_grad,
        bounds=bounds,
        sigma_bounds=sigma_bounds,
        patience=patience,
    )

    logger.info(
        f"Chosen prior prec = {result['prior_prec']:.4f}, sigma = {result['sigma']:.4f}"
    )
    return result
